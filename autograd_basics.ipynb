{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "autograd_basics.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritukuklani/Deep-Learning/blob/main/autograd_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXjHKkRgAYdd"
      },
      "source": [
        "# CS-GY 9223 Deep Learning Recitation 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScIcmXvfAYde"
      },
      "source": [
        "<i>Referenced from course material of DS-GA 1008, NYU CDS. <br> Instructors: Yann LeCun & Alfredo Canziani <br> https://atcold.github.io/pytorch-Deep-Learning/  </i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wka2Z1c4AYde"
      },
      "source": [
        "# Autograd: automatic differentiation\n",
        "\n",
        "The ``autograd`` package provides automatic differentiation for all operations\n",
        "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
        "defined by how your code is run, and that every single iteration can be\n",
        "different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oepTX_e6AYdf"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXNah-i5AYdh"
      },
      "source": [
        "Create a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ86FclGAYdh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "56c11f0d-fe62-4094-c20e-d4d09c596d8b"
      },
      "source": [
        "# Create a 2x2 tensor with gradient-accumulation capabilities\n",
        "x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 2.],\n",
            "        [3., 4.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMbUKP4aAYdk"
      },
      "source": [
        "Do an operation on the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBQrHsWGAYdk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "2e0305ce-db54-49e7-9342-216fe14c6c99"
      },
      "source": [
        "# Deduct 2 from all elements\n",
        "y = x - 2\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.,  0.],\n",
            "        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6LqDLcHAYdm"
      },
      "source": [
        "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8-01ipHAYdm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c1f3c22-2dba-48b0-ad87-bd39f9c83374"
      },
      "source": [
        "print(y.grad_fn) # We have a gradient function for y as it has been generated from x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SubBackward0 object at 0x7f90ed6eb7f0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHVLKV5XAYdo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f69c4fbb-5517-4d5e-fb0e-2a1d7b4dd70a"
      },
      "source": [
        "# What's happening here?\n",
        "print(x.grad_fn) # We did not get a gradient_function for x as no backpropogation has been performed for generate x. We have just declared it as a variable above."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SUVhpRSAYdq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad83b427-728e-4bdb-eb81-ed271f4e2437"
      },
      "source": [
        "# Let's dig further...\n",
        "y.grad_fn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SubBackward0 at 0x7f90ed6eb2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IMb18plAYdt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d477cbf5-8d69-47cc-a978-0eb47fec9f4d"
      },
      "source": [
        "y.grad_fn.next_functions[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AccumulateGrad at 0x7f90a380f208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOPUjDKsAYdv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3c0b6cb5-59f3-4c2a-f4f6-20312c46bb6a"
      },
      "source": [
        "y.grad_fn.next_functions[0][0].variable"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68QPUF6xAYdw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a8645dde-fb19-42a4-b998-d098650d8f85"
      },
      "source": [
        "# Do more operations on y\n",
        "z = y * y * 3\n",
        "a = z.mean()  # average\n",
        "\n",
        "print(z)\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 3.,  0.],\n",
            "        [ 3., 12.]], grad_fn=<MulBackward0>)\n",
            "tensor(4.5000, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs7faaiWBqJb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "1f815575-aff9-460c-82a2-846935cfb946"
      },
      "source": [
        "!pip install torchviz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchviz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/8e/a9630c7786b846d08b47714dd363a051f5e37b4ea0e534460d8cdfc1644b/torchviz-0.0.1.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.6.0+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.18.5)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.1-cp36-none-any.whl size=3520 sha256=9d1a3408c4a2e1870cdf795b069cc67a2ea42aeba28bea73c76d491b9c6e5328\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/c2/c5/b8b4d0f7992c735f6db5bfa3c5f354cf36502037ca2b585667\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6yqwj1wAYdy"
      },
      "source": [
        "# Let's visualise the computational graph! (thks @szagoruyko)\n",
        "from torchviz import make_dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSPljFqJAYd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "1d7b7943-6c9d-42bd-900a-d0a2c4eee398"
      },
      "source": [
        "make_dot(a) # The yellow box indicates the starting point of the graph(a). Gradients are computed backwards. \n",
        "            # The blue box is the actual gradient matrix. The gray boxes are the intermediate gradients. \n",
        "            # The two arrows indicate y*y and the next mulbackward0 indicates *3.\n",
        "            # Pytorch doesn't create gradients for nodes that do not contribute in the gradient calculation."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f90ed6eb208>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"106pt\" height=\"271pt\"\n viewBox=\"0.00 0.00 106.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-267 102,-267 102,4 -4,4\"/>\n<!-- 140260435473464 -->\n<g id=\"node1\" class=\"node\">\n<title>140260435473464</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"98,-21 0,-21 0,0 98,0 98,-21\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MeanBackward0</text>\n</g>\n<!-- 140260435473800 -->\n<g id=\"node2\" class=\"node\">\n<title>140260435473800</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94.5,-78 3.5,-78 3.5,-57 94.5,-57 94.5,-78\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140260435473800&#45;&gt;140260435473464 -->\n<g id=\"edge1\" class=\"edge\">\n<title>140260435473800&#45;&gt;140260435473464</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49,-56.7787C49,-49.6134 49,-39.9517 49,-31.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-31.1732 49,-21.1732 45.5001,-31.1732 52.5001,-31.1732\"/>\n</g>\n<!-- 140260878573072 -->\n<g id=\"node3\" class=\"node\">\n<title>140260878573072</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94.5,-135 3.5,-135 3.5,-114 94.5,-114 94.5,-135\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 140260878573072&#45;&gt;140260435473800 -->\n<g id=\"edge2\" class=\"edge\">\n<title>140260878573072&#45;&gt;140260435473800</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49,-113.7787C49,-106.6134 49,-96.9517 49,-88.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-88.1732 49,-78.1732 45.5001,-88.1732 52.5001,-88.1732\"/>\n</g>\n<!-- 140260435473072 -->\n<g id=\"node4\" class=\"node\">\n<title>140260435473072</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"94,-192 4,-192 4,-171 94,-171 94,-192\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">SubBackward0</text>\n</g>\n<!-- 140260435473072&#45;&gt;140260878573072 -->\n<g id=\"edge3\" class=\"edge\">\n<title>140260435473072&#45;&gt;140260878573072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M43.5885,-170.7787C42.3317,-163.6134 41.9599,-153.9517 42.4733,-145.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"45.9738,-145.498 43.597,-135.1732 39.0164,-144.7267 45.9738,-145.498\"/>\n</g>\n<!-- 140260435473072&#45;&gt;140260878573072 -->\n<g id=\"edge5\" class=\"edge\">\n<title>140260435473072&#45;&gt;140260878573072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M54.4115,-170.7787C55.6683,-163.6134 56.0401,-153.9517 55.5267,-145.3097\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"58.9836,-144.7267 54.403,-135.1732 52.0262,-145.498 58.9836,-144.7267\"/>\n</g>\n<!-- 140259195154952 -->\n<g id=\"node5\" class=\"node\">\n<title>140259195154952</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"76,-263 22,-263 22,-228 76,-228 76,-263\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (2, 2)</text>\n</g>\n<!-- 140259195154952&#45;&gt;140260435473072 -->\n<g id=\"edge4\" class=\"edge\">\n<title>140259195154952&#45;&gt;140260435473072</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M49,-227.6724C49,-219.8405 49,-210.5893 49,-202.4323\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"52.5001,-202.2234 49,-192.2234 45.5001,-202.2235 52.5001,-202.2234\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAk9aXwkAYd2"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1fQNdoiAYd3"
      },
      "source": [
        "# Backprop\n",
        "a.backward()    # da/dx = 1.5*xi-3 (No derivative for summation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9LjuEVSAYd4"
      },
      "source": [
        "Print gradients $\\frac{\\text{d}a}{\\text{d}x}$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn59mxpIAYd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e0b031fe-4fac-46a8-ff75-19d1ef8994c2"
      },
      "source": [
        "# Compute it by hand BEFORE executing this\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.5000,  0.0000],\n",
            "        [ 1.5000,  3.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnDCTGnnAYd6"
      },
      "source": [
        "You can do many crazy things with autograd!\n",
        "> With Great *Flexibility* Comes Great Responsibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYXA79lFAYd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "05b0991b-243a-4c87-bd65-b73667d6e1d4"
      },
      "source": [
        "# Dynamic graphs!\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x * 2\n",
        "print(y)\n",
        "i = 0\n",
        "print(y.data.norm)\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "    i += 1\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-0.2742,  0.4320, -0.2103], requires_grad=True)\n",
            "tensor([-0.5484,  0.8641, -0.4206], grad_fn=<MulBackward0>)\n",
            "<bound method Tensor.norm of tensor([-0.5484,  0.8641, -0.4206])>\n",
            "tensor([-561.5914,  884.8142, -430.6466], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNwCtjjoAYd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adc58db0-dc97-442a-c03a-33da162fa4b1"
      },
      "source": [
        "# If we don't run backward on a scalar we need to specify the grad_output\n",
        "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
        "y.backward(gradients)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoEUECtYAYd_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37000244-ce98-400b-f3e5-54203020b079"
      },
      "source": [
        "# BEFORE executing this, can you tell what would you expect it to print?\n",
        "print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_EFwj7SAYeC"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc3UmXTpAYeC"
      },
      "source": [
        "# This variable decides the tensor's range below\n",
        "n = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRJblk4IAYeE"
      },
      "source": [
        "# Both x and w that allows gradient accumulation\n",
        "x = torch.arange(1., n + 1, requires_grad=True)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJoqdlpxAYeF"
      },
      "source": [
        "# Only w that allows gradient accumulation\n",
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "z = w @ x\n",
        "z.backward()\n",
        "print(x.grad, w.grad, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFozNsoeAYeH"
      },
      "source": [
        "x = torch.arange(1., n + 1)\n",
        "w = torch.ones(n, requires_grad=True)\n",
        "\n",
        "# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n",
        "with torch.no_grad():\n",
        "    z = w @ x\n",
        "\n",
        "try:\n",
        "    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n",
        "except RuntimeError as e:\n",
        "    print('RuntimeError!!! >:[')\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHhhEtksAYeI"
      },
      "source": [
        "## More stuff\n",
        "\n",
        "Documentation of the automatic differentiation package is at\n",
        "http://pytorch.org/docs/autograd."
      ]
    }
  ]
}